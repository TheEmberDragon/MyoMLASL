# -*- coding: utf-8 -*-
"""Tutorial-Hello, TensorFlow!.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WhaKcaOoVWEvBykA4o60bV7VOg9gdimg

# New to TensorFlow?
# Start here!
"""

"""**Basic Neural Network** - TensorFlow
Implementation of a NN with back propagation using TensorFlow only.
"""

import fnmatch
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics

pathDataFolder = '../data'

num_periods = 250
inputs = 12
outputs = 6
hidden = 300

# initialize input dataset (matrix)
# - each row is a training example - 4 examples 
# - each col is a different neuron - 3 neurons
X = []
    
letters = ['a', 'b', 'c', 'e', 'f', 'o']
def get_positive_label(letter):
    output_label = [0] * len(letters)
    output_label[letters.index(letter)] = 1
    return output_label


# initialize output dataset 
# - one output neuron for each training example
Y = []
    
for i, letter in enumerate(letters):
    for file_name in fnmatch.filter(os.listdir(pathDataFolder), "[0-9]-{0}*.csv".format(letter)):
        X.append(np.loadtxt(pathDataFolder+'/'+file_name, delimiter=','))
        Y.append([get_positive_label(letter)] * 250)

# Split into training set and testing set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                test_size=0.1, train_size=0.9, random_state=None)

import tensorflow as tf
# set up TF - tensors needed for the inputs
# - layer 0 is the input data
x = tf.placeholder(tf.float32, [None, num_periods, inputs])
y = tf.placeholder(tf.float32, [None, num_periods, outputs])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden, activation=tf.nn.relu)
rnn_output, states = tf.nn.dynamic_rnn(basic_cell, x, dtype=tf.float32)

learning_rate = 0.001

stacked_rnn_output = tf.reshape(rnn_output, [-1, num_periods, hidden])
stacked_outputs = tf.layers.dense(stacked_rnn_output, outputs)
outputs = tf.reshape(stacked_outputs, [-1, num_periods, outputs])

loss = tf.reduce_sum(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
trainint_op = optimizer.minimize(loss)

epochs = 1000

with tf.Session() as sess:
    
    sess.run( tf.global_variables_initializer() )
    for ep in range(epochs):
        sess.run( trainint_op, feed_dict={x: X_train, y: Y_train})

    y_pred = sess.run(outputs, feed_dict={x: X_test})

    predict = y_pred[:, 249]
    
    truth = np.array(Y_test)[:, 249]
    # new_list = []
    # for p in predict:
    #     new = [0, 0, 0, 0, 0, 0]
    #     new[np.argmax(p)] = 1
    #     new_list.append(new)
    
    print("Test set results:")
    print("------------------------------------------------")
    print(metrics.confusion_matrix(truth.argmax(axis=1), predict.argmax(axis=1)))
    print('accuracy:    ', metrics.accuracy_score(truth.argmax(axis=1), predict.argmax(axis=1)))
    print('f_1:         ', metrics.f1_score(truth.argmax(axis=1), predict.argmax(axis=1), average=None))
    print('recall:      ', metrics.recall_score(truth.argmax(axis=1), predict.argmax(axis=1), average=None))
    print('precision:   ', metrics.precision_score(truth.argmax(axis=1), predict.argmax(axis=1), average=None))
    print('kappa:       ', metrics.cohen_kappa_score(truth.argmax(axis=1), predict.argmax(axis=1)))